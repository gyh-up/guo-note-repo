# IO 模型

IO多路复用多用于解决高性能服务器的并发问题（`Redis`、`Nginx`  底层就是用  `epoll`  实现）

参考文档：

https://www.xttblog.com/?p=5186
https://blog.csdn.net/flying_monkey_1/article/details/115558263
https://blog.csdn.net/weixin_39662142/article/details/110396979

https://www.bilibili.com/video/BV12i4y1G7UK/?spm_id_from=333.880.my_history.page.click&vd_source=2365b1baf9eff4a0af3aabdb705341bd

## 1、BIO 

- `BIO` 中的 `B` 是 `Blocking` 的阻塞的意思

- 作为服务端开发，使用 `ServerSocket` 绑定端口号之后会监听该端口，等待 `accept` 事件，`accept` 是会阻塞当前线程

- 当我们收到 `accept` 事件的时候，程序就会拿到客户端与当前服务端连接的`Socket`

- 针对这个 `socket` 我们可以进行读写，但是，这个 `socket` 读写都是会阻塞当前线程的。

- 一般我们会有使用**多线程方式进行 `c/s` 交互，但是这样很难做到 `C10K`**。比如说：`1W` 个客户端就需要和服务端用 `1W` 个线程支持，这样的话CPU肯定就爆炸了，同时线程上下文切换也会把机器负载给拉飞，操作系统是扛不住这么大开销的。。

  `BIO` 伪代码：

  ```
  listenfd = socket();   // 打开一个网络通信端口
  bind(listenfd);        // 绑定
  listen(listenfd);      // 监听
  while(1) {
    connfd = accept(listenfd);  // 阻塞建立连接
    int n = read(connfd, buf);  // 阻塞读数据
    doSomeThing(buf);  // 利用读到的数据做些什么
    close(connfd);     // 关闭连接，循环等待下一个连接
  }
  ```

  

## 2、NIO 

服务器端当 `accep`t 一个请求后，加入 `fds` 集合，每次轮询一遍 `fds` 集合 `recv` （非阻塞）数据，没有数据则立即返回错误，每次轮询所有 `fd` （包括没有发生读写实际的` fd`）会很浪费 `CPU`。

```
// 伪代码描述
while(true) {  
    // accept非阻塞(cpu一直轮询)  
    client_fd = accept(listen_fd)  
    if (client_fd != null) {    
        // 有人连接 
        fds.append(client_fd)  
    } else {    
        // 无人连接  
    }    
    for (fd in fds) {    
        // recv非阻塞    
        setNonblocking(client_fd)    
        // recv 为非阻塞命令    
        if (len = recv(fd) && len > 0) {      
        	// 有读写数据      
        	logic（）    
        } else {       
        	// 无读写数据    
        }  
    }  
}
```



## 3、IO多路复用

通常情况下，对文件描述符进行读写操作时，函数会将某一个文件描述符作为参数，即单个函数操作一个文件。很多应用场景下，如单线程服务器，需要某个函数实现监听多个文件描述符的功能。通俗来说，**多路复用就是一个进程监听多个文件描述符**。

`select`、`poll`、`epoll` 是内核提供给用户态的系统调用，**进程（用户态）可以通过系统调用函数**，从内核中获取某个 `socket` 文件描述符是否有读写事件。那 `select`、`poll`、`epoll` 如何获取网络事件的呢？进程（用户态）先把 `socket` 文件描述符传给内核，再由内核返回产生了事件的 `socket`，最后 由用户态处理这些 `socket` 对应的请求。

## 4、select

`select` 知道有 `I/O` 事件发生，却并不知道是哪几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以 **select 具有 O(n) 的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

- 每次调用 `kernel` 的 `select` 函数，都会涉及到用户态/内核态的切换，还需要传递需要检查的 `socket` 集合，其实就是需要检查的 ` fd` （文件描述符）集合。因为程序都是运行在 `linux` 或者 `unix` 操作系统上，在这种操作系统中一切皆文件，`socket` 也不例外，这里传递的 `fd` 其实就是文件系统中对应 `socket` 生成的文件描述符
- 操作系统 这个 `select` 函数被调用以后，
  - **首先会去  `fd` 集合中去检查内存中 `socket` 套接字的状态，这个时间复杂度是 `O(N)`的**，然后循环遍历一遍之后，如果有就绪状态的 `socket`，那么就会直接返回，不会阻塞当前线程。
  - 否则的话，那个就说明当前指定 `fd` 集合对应的 `socket` 没有就绪状态，那么就需要阻塞当前调用线程，直到有某个 `socket` 有数据之后，才唤醒线程。

- **select 对监听 socket 有 1024 的大小限制**
- 实际上是 1021 个，每一个 Linux 进程，启动时会新建三个文件描述符 0，1，2，它们为标准输入，标准输出，标准错误
  - 这个是因为 `fd` 集合这个结构是一个 `bitmap` 位图的结构，这个位图结构就是一个长的二进制数，类似 `0101 0101`
  
- 这个 `bitmap` 默认长度是 `1024个bit`，因为是操作系统源码默认的值，想要修改长度非常麻烦，需要**重新编译操作系统内核**
  
- 处于某种性能考虑，`select` 函数做了两件事

  - **第一件事**，跑到就绪状态的 `socket` 对应的 `fd` 文件中设置一个标记 `mask`（置位），表示这个 `fd` 对应的 `socket` 就绪了
  - 第二件事，返回 `select`   函数，对应的也就是唤醒 `java`  线程，站在 `java` 层面，他会收到一个 `int` 结果值，表示有几个`socket` 处于就绪状态。
  - 但是，具体是哪个 `socket` 就绪，`java` 是不知道的，所以接下来会是一个 `O(N)` 的系统调用，检查 `fd` 集合中每一个 `socket` 的就绪状态，其实就是检查文件系统中指定 `socket` 的文件描述符的状态，**涉及到用户态和内核态的来回切换，如果 `bitmap`再大，就需要更多的系统调用，还有就是系统调用涉及到参数的数据拷贝**，如果数据太庞大，也不利于系统的调用速度。

## 5、select 深入问题

**问题：`select` 第一遍 `O(N)` 去检查未发现就绪的 `socket` ，后续某个 `socket` 就绪后，`select` 是如何感知道的？是不断的轮询吗？**

> **铺垫知识**

- **操作系统调度**
  - `cpu`  同一时刻，它只能运行一个进程，操作系统做主要的任务就是系统调度，就是有 `n` 个线程，然后让这 `n` 个线程在 `cpu` 上切换进行
  - **未挂起的线程都在工作队列内**，都有机会获取到 `cpu` 的执行权
  - **挂起的线程就会从这个工作队列里移除出去**，反映到咱们 `java` 层面就是线程阻塞了
  - `linux` 系统线程其实就是轻量级进程

- **操作系统中断**
  - 比如说，咱们用键盘打字，如果 `cpu` 正执行其他程序，一直不释放，那咱这个打字就也没法打了
  - 咱们都知道，不是这样的情况，因为就是有系统中断的存在，当按下一个键以后会给主板发送一个电流信号，主板感知到以后，它就会触发这个 `cpu` 中断、
  - **所以中断 其实就是 `cpu` 给正在执行的进程先保留程序上下文，然后避让出 `cpu`，给中断程序绕道**
  - **中断程序就会拿到 `cpu` 的执行权限**，进行相应代码的执行，比如说键盘的中断程序，就会执行输出的逻辑

> **回到最开始的问题**

- 如果 `select` 函数第一遍轮询，没有发现就绪状态的 `socket` 的话，它就会把当前进程保留给需要检查的 `socket` 等待队列中
- `socket`  结构 有三块核心区域，分别就是**读缓存，写缓存还有这个等待队列**
- 这个 `select` 函数，它会把当前进程保留到每个需要检查的 `socke` 的等待队列中，就会把当前进程从工作队列里面移除了，移除了之后其实就是挂起了当前线程，然后这个 `select` 函数也就不会再运行了
- **下一个阶段，假设我们客户端往当前服务器发送了数据**，数据通过网线到网卡，网卡再到 `DMA` 硬件的这种方式直接将数据写到内存里面，然后整个过程，`CPU` 是不参与的
- 当传输完成以后，它就会触发网络数据传输完毕的中断程序，这个中断程序它会把 `cpu` 正在执行的进程给顶掉，然后 `cpu` 就会执行咱这个中断程序的逻辑
  - 对应的逻辑是：根据内存中的数据包，然后分析出来数据包是哪个 `socket` 的数据，
  - 同时 `tcp/ip` 它又是保证传输的时候是有端口号的，然后根据端口号就能找到对应的 `socket` 实例，找到 `socket` 实例以后，就会**把数据导入到 `socket` 读缓冲里面**
  - **导入完成以后，它就开始去检查 `socket` 等待队列**，看是不是有等待者，如果有等待者的话，就会把等待者移动到工作队列里面去，中断程序到这一步就执行完了
  - 这样咱们的进程就又回到了工作队列，又有机会获取到 `cpu` 时间片了
- 然后当前进程执行的 `select` 函数再次检查，就会发现这个就绪的 `socket`，就会给就绪的 `socket` 的 `fd` 文件描述符打标记，然后 `select` 函数就执行完了，返回到 `java` 层面就涉及到内核态和用户态的转换，**后面的事情就是轮询检查每一个 `socket` 的 `fd`  是否被打了标记，然后就是处理被打了标记的 `socket` 就 `ok` 了**

## 6、poll 

`poll` 本质上和 `select` 没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个 `fd` 对应的设备状态， **但是它没有最大连接数的限制**，原因是它是基于链表来存储。

- 传参不一样
  - **`select` 用的是 `bitmap` ，**它表示需要检查的 `socket` 集合
  - **`poll` 使用的是链表结构，**表示需要检查的 `socket` 集合（主要是为了解决 `socket`  监听长度不超过`1024` 的限制）

## 7、epoll 

**`epoll` 可以理解为 `event poll`**，不同于忙轮询和无差别轮询，`epoll` 会把哪个流发生了怎样的 `I/O` 事件通知我们。所以我们说 `epoll` 实际上是**事件驱动（每个事件关联上 `fd`）**的，此时我们对这些流的操作都是有意义的，**复杂度降低到了 `O(1)`。**

- `select` 和 `poll` 的共有缺陷

  - 第一个缺陷：`select`  和 `poll`函数，这两系统函数每次调用**都需要我们提供给它所有的需要监听的 `socket`  文件描述符集合，而且主线程是死循环调用 `select/poll` 函数的，这里面涉及到用户空间数据到内核空间拷贝的过程**

    - 咱们需要监听的 `socket` 集合，**数据变化非常小**
    - **每次就一到两个 `socket_fd` 需要更改，但是没有办法，因为 `select` 和 `poll` 函数，只是一个很单纯的函数**
    - **它在 `kernel` 层面，不会保留任何的数据信息，所以说每次调用都进行了数据拷贝**

  - 第二个缺陷：

    `select` 和 `poll` 函数它的返回值都是 `int` 整型值，只能代表有几个 `socket` 就绪或者有错误了，它没办法表示具体是哪个`socket` 就绪了。这就**导致了程序被唤醒以后，还需要新的一轮系统调用去检查哪个 `socket` 是就绪状态的**，然后再进行 `socket` 数据处理逻辑，这里走了不少弯路（同时还存在用户态和内核态的切换，这样缺陷就更严重了）

**`epoll` 就是为了解决这两个问题**

## 8、epoll 实现原理

当某一进程调用 `epoll_create` 方法时，`Linux` 内核会创建一个 `eventpoll` 结构体，这个结构体中有两个成员与 `epoll` 的使用方式密切相关。`eventpoll` 结构体如下所示：

```php
#include 
// 数据结构
// 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件
// epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可
struct eventpoll {    
    struct rb_root  rbr;    /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/    
    struct list_head rdlist;	/*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/    
};
// API
int epoll_create(int size); 
// epoll_create 负责在内核中间加一个 ep 对象，把所有需要监听的 socket 都放到 ep 对象中
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 
// epoll_ctl 负责把 socket 增加、删除到内核红黑树
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
// epoll_wait 负责检测可读队列，默认情况下会阻塞系统的调用线程，直到`eventpoll` 对象中关联的某个或者某些个socket就绪以后，`epoll_wait`函数才会返回。返回0，表示没有就绪的socket，返回大于0，表示有几个就绪的socket，返回-1表示异常
```

`epoll` 的用法了，一句话描述就是：三步曲。

- 第一步：`epoll_create()` 系统调用。此调用返回一个句柄，之后所有的使用都依靠这个句柄来标识。
- 第二步：`epoll_ctl()` 系统调用。通过此调用向 `epoll` 对象中添加、删除、修改感兴趣的事件，返回 `0` 标识成功，返回 `-1` 表示失败。
- 第三部：`epoll_wait()` 系统调用。通过此调用收集在 `epoll` 监控中已经发生的事件。

## 9、eventpoll 对象就绪列表的维护

**`select` 函数调用的流程：**

- `socket` 对象有三块区域
  - 读缓冲区
  - 写缓冲区
  - 等待队列
- `select` 函数调用的时候会把当前进程从工作队列里面拿出来
- **然后把进程引用追加到当前进程关注的每一个 `socket` 对象的等待队列中**
- 然后当 `socket` 连接的客户端发送完数据之后，数据还是通过硬件 `DMA` 的方式把数据写入到内存，然后相应的硬件向 `CPU` 发出中断信号，`CPU` 就会让出当前进程位置去执行网络数据就绪的中断程序，
- **这个中断程序就会把内存中的网络数据写入到对应的 `socket` 读缓冲区里面，之后把这个 `socket` 等待队列中的进程全部移动到工作队列中，再然后 `select` 函数返回**

**`epoll` 函数流程非常相似**

- 当我们调用系统函数 `epoll_ ctl`时候，比如我们新添加一个需要关注的 `socket`，其实**内核程序会把当前的 `eventpoll` 对象追加到这个 `socket` 的等待队列里头**
- 然后当 `socket` 连接的客户端发送完数据之后，数据还是通过硬件 `DMA` 的方式把数据写入到内存，然后相应的硬件向 `CPU` 发出中断信号，`CPU` 就会让出当前进程位置去执行网络数据就绪的中断程序，
- **这个中断程序就会把内存中的网络数据写入到对应的 `socket` 读缓冲区里面，然后它发现这个 `socket` 的等待队列里头不是进程，而是一个 `eventpoll` 对象的引用**
- **这个时候呢，他就会根据这个 `eventpoll` 对象的引用，将当前 `socket` 的引用追加到 `eventpoll` 的就绪链表的末尾**（`eventpoll` 还有一块空间是`eventpoll` 的等待队列，这个等待队列保存的就是调用 `epoll_wait` 的进程）
- 然后，当中断程序把 `socket` 的引用追加到就绪列表的末尾之后，就继续检查 `eventpoll` 对象的等待队列，如果有进程，就会把进程转移到工作队列中
- 转移完毕之后，进程就有获取到 `CPU` 执行的时间片了，然后就是调用 `epoll_wait` 函数，他这个函数就返回到java层面了

**总结：**

- **`eventpoll` 对象等待队列里面，它有调用 `epoll_wait(...) ` 函数进去的进程**
- **然后再把这个进程，从这个 `eventpoll` 的等待队列里面迁移到工作队列里面**

## 10、epoll_wait() 获取就绪的 socket

**`epoll_wait()`  返回值是 Int` 类型的**

- 返回 `0`，表示没有就绪的 `socket`
- 返回大于 `0`，表示有几个就绪的 `socket`
- 返回 `-1` 表示异常

**那么获取就绪的 `socket` 是怎么实现的呢？**

- `epoll_wait` 函数，**调用的时候**会传入一个 `epoll_event` 事件数组指针
- `epoll_wait` **函数正常返回之前**，会把就绪的 `socket` 事件信息拷贝到这个数组指针里头
- 这样返回到上层程序，就能通过这个数组拿到就绪列表

## 11、epoll_wait 可不可以设置成非阻塞的

- 默认 `epoll_wait` 是阻塞的
- 它有一个参数，表示阻塞时间的长度，如果**这个参数设置为0**，表示这个 `epoll_wait` 是一个非阻塞调用的
- **每次调用都会去检查就绪列表**

## 12、epoll LT 与 ET 模式的区别

`epoll` 有 `EPOLLLT` 和 `EPOLLET` 两种触发模式，LT 是默认的模式，ET 是 “高速” 模式。

- `LT` 模式下，只要这个 `fd` 还有数据可读，每次 `epoll_wait` 都会返回它的事件，提醒用户程序去操作；
- `ET` 模式下，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无论 `fd` 中是否还有数据可读。所以在 `ET` 模式下，`read` 一个 `fd` 的时候一定要把它的 `buffer` 读完，或者遇到 `EAGIN` 错误。

`epoll` “事件”的就绪通知方式，通过 `epoll_ctl` 注册 `fd`，一旦该 `fd` 就绪，内核就会采用类似 `callback` 的回调机制来激活该 `fd`， `epoll_wait` 便可以收到通知。

## 13、支持一个进程所能打开的最大连接数

- `select`：单个进程所能打开的最大连接数有 `FD_SETSIZE` 宏定义，其大小是 `32` 个整数的大小（在 `32` 位的机器上，大小就是`32_32` ，同理 `64` 位机器上 `FD_SETSIZE` 为 `32_64`），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试。
- `poll`：`poll` 本质上和 `select` 没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的。
- `epoll`：虽然连接数有上限，但是很大，`1G`内存的机器上可以打开 `10` 万左右的连接，`2G` 内存的机器可以打开 `20` 万左右的连接。

## 14、消息传递方式

- `select`：内核需要将消息传递到用户空间，都需要内核拷贝动作
- `poll`：同上
- `epoll`：通过内核和用户空间共享一块内存来实现的。

## 15、nginx 所使用的IO模型是什么？

`Nginx` 支持多种并发模型，并发模型的具体实现根据系统平台而有所不同。

在支持多种并发模型的平台上，`nginx` 自动选择最高效的模型。但我们也可以使用 `use` 指令在配置文件中显式地定义某个并发模型。

**`NGINX` 中支持的并发模型：**

- `select`

`IO` 多路复用、标准并发模型。在编译 `nginx` 时，如果所使用的系统平台没有更高效的并发模型，`select` 模块将被自动编译。`configure` 脚本的选项：`–with-select_module 和 --without-select_module` 可被用来强制性地开启或禁止 `select` 模块的编译

- `poll`

`IO` 多路复用、标准并发模型。与 `select` 类似，在编译 `nginx` 时，如果所使用的系统平台没有更高效的并发模型，`poll` 模块将被自动编译。`configure` 脚本的选项：`–with-poll_module` 和 `--without-poll_module 可用于强制性地开启或禁止 poll 模块的编译

- `epoll`

IO多路复用、高效并发模型，可在 Linux 2.6+ 及以上内核可以使用

- `kqueue`

IO多路复用、高效并发模型，可在 `FreeBSD 4.1+`，`OpenBSD 2.9+`，`NetBSD 2.0`，和 `Mac OS X` 平台中使用

- `/dev/poll`

高效并发模型，可在 `Solaris 7 11/99+`，`HP/UX 11.22+ (eventport)`，`IRIX 6.5.15+`，和 `Tru64 UNIX 5.1A+` 平台使用

- `eventport`

高效并发模型，可用于 `Solaris 10` 平台，PS：由于一些已知的问题，建议 使用 `/dev/poll` 替代。

## 总结

`select`，`poll`实现需要自己不断轮询所有 `fd` 集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而 `epoll` 其实也需要调用 `epoll_wait` 不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪 `fd` 放入就绪链表中，并唤醒在 `epoll_wait` 中进入睡眠的进程。虽然都要睡眠和交替，但是 `select` 和 `poll` 在“醒着”的时候要遍历整个 `fd` 集合，而 `epoll` 在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的 `CPU` 时间。这就是回调机制带来的性能提升。

`select`，`poll` 每次调用都要把 `xfd` 集合从用户态往内核态拷贝一次，并且要把 `current` 往设备等待队列中挂一次，而 `epoll` 只要一次拷贝，而且把 `current` 往等待队列上挂也只挂一次（在 `epoll_wait` 的开始，注意这里的等待队列并不是设备等待队列，只是一个`epoll` 内部定义的等待队列）。这也能节省不少的开销。