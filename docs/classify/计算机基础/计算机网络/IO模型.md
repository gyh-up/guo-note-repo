# IO 模型

IO多路复用多用于解决高性能服务器的并发问题（`Redis`、`Nginx`  底层就是用  `epoll`  实现）

## 1、BIO 

- `BIO` 中的 `B` 是 `Blocking` 的阻塞的意思

- 作为服务端开发，使用 `ServerSocket` 绑定端口号之后会监听该端口，等待 `accept` 事件，`accept` 是会阻塞当前线程

- 当我们收到 `accept` 事件的时候，程序就会拿到客户端与当前服务端连接的`Socket`

- 针对这个 `socket` 我们可以进行读写，但是，这个 `socket` 读写都是会阻塞当前线程的。

- 一般我们会有使用**多线程方式进行 `c/s` 交互，但是这样很难做到 `C10K`**。比如说：`1W` 个客户端就需要和服务端用 `1W` 个线程支持，这样的话CPU肯定就爆炸了，同时线程上下文切换也会把机器负载给拉飞，操作系统是扛不住这么大开销的。。

  `BIO` 伪代码：

  ```
  listenfd = socket();   // 打开一个网络通信端口
  bind(listenfd);        // 绑定
  listen(listenfd);      // 监听
  while(1) {
    connfd = accept(listenfd);  // 阻塞建立连接
    int n = read(connfd, buf);  // 阻塞读数据
    doSomeThing(buf);  // 利用读到的数据做些什么
    close(connfd);     // 关闭连接，循环等待下一个连接
  }
  ```

  

## 2、NIO 

服务器端当 accept 一个请求后，加入 fds 集合，每次轮询一遍 fds 集合 recv (非阻塞)数据，没有数据则立即返回错误，每次轮询所有 fd (包括没有发生读写实际的 fd)会很浪费 CPU。

```
// 伪代码描述
while(true) {  
    // accept非阻塞(cpu一直忙轮询)  
    client_fd = accept(listen_fd)  
    if (client_fd != null) {    
        // 有人连接 
        fds.append(client_fd)  
    } else {    
        // 无人连接  
    }    
    for (fd in fds) {    
        // recv非阻塞    setNonblocking(client_fd)    
        // recv 为非阻塞命令    
        if (len = recv(fd) && len > 0) {      
        // 有读写数据      
        // logic    
        } else {       
        // 无读写数据    
        }  
    }  
}
```



## 3、select 实现原理

- 每次调用 `kernel` 的 `select` 函数，都会涉及到用户态/内核态的切换，还需要传递需要检查的`socket` 集合，其实就是需要检查的 ` fd` （文件描述符）集合。因为程序都是运行在`linux` 或者 `unix` 操作系统上，在这种操作系统中一切皆文件，`socket` 也不例外，这里传递的 `fd` 其实就是文件系统中对应 `socket` 生成的文件描述符
- 操作系统 这个 `select` 函数被调用以后，
  - **首先会去  `fd` 集合中去检查内存中 `socket` 套接字的状态，这个时间复杂度是 `O(N)`的**，然后循环遍历一遍之后，如果有就绪状态的 `socket`,那么就会直接返回，不会阻塞当前线程。
  - 否则的话，那个就说明当前指定 `fd` 集合对应的 `socket` 没有就绪状态，那么就需要阻塞当前调用线程，直到有某个 `socket` 有数据之后，才唤醒线程。

- **select 对监听socket有1024的大小限制**

  - 实际上是 1021 个，每一个 Linux 进程，启动时会新建三个文件描述符 0，1，2，它们为标准输入，标准输出，标准错误
  - 这个是因为 `fd` 集合这个结构是一个 `bitmap` 位图的结构，这个位图结构就是一个长的二进制数，类似 `0101 0101`

  - 这个 `bitmap` 默认长度是 `1024个bit`，因为是操作系统源码默认的值，想要修改长度非常麻烦，需要**重新编译操作系统内核**

- 处于某种性能考虑，`select` 函数做了两件事

  - **第一件事**，跑到就绪状态的 `socket` 对应的 `fd` 文件中设置一个标记 `mask`（置位），表示这个 `fd` 对应的 `socket` 就绪了
  - 第二件事，返回 `select`   函数，对应的也就是唤醒 `java`  线程，站在 `java` 层面，他会收到一个 `int` 结果值，表示有几个`socket` 处于就绪状态。
  - 但是，具体是哪个 `socket` 就绪，`java` 是不知道的，所以接下来会是一个 `O(N)` 的系统调用，检查 `fd` 集合中每一个 `socket` 的就绪状态，其实就是检查文件系统中指定 `socket` 的文件描述符的状态，**涉及到用户态和内核态的来回切换，如果 `bitmap`再大，就需要更多的系统调用，还有就是系统调用涉及到参数的数据拷贝**，如果数据太庞大，也不利于系统的调用速度。

## 4、 select 深入问题

**问题：`select` 第一遍 `O(N)` 去检查未发现就绪的 `socket` ，后续某个 `socket` 就绪后，`select` 是如何感知道的？是不断的轮询吗？**

> **铺垫知识**

- **操作系统调度**
  - `cpu`  同一时刻，它只能运行一个进程，操作系统做主要的任务就是系统调度，就是有 `n` 个线程，然后让这 `n` 个线程在 `cpu` 上切换进行
  - **未挂起的线程都在工作队列内**，都有机会获取到 `cpu` 的执行权
  - **挂起的线程就会从这个工作队列里移除出去**，反映到咱们 `java` 层面就是线程阻塞了
  - `linux` 系统线程其实就是轻量级进程

- **操作系统中断**
  - 比如说，咱们用键盘打字，如果 `cpu` 正执行其他程序，一直不释放，那咱这个打字就也没法打了
  - 咱们都知道，不是这样的情况，因为就是有系统中断的存在，当按下一个键以后会给主板发送一个电流信号，主板感知到以后，它就会触发这个 `cpu` 中断、
  - **所以中断 其实就是 `cpu` 给正在执行的进程先保留程序上下文，然后避让出 `cpu`，给中断程序绕道**
  - **中断程序就会拿到 `cpu` 的执行权限**，进行相应代码的执行，比如说键盘的中断程序，就会执行输出的逻辑

> **回到最开始的问题**

- 如果 `select` 函数第一遍轮询，没有发现就绪状态的 `socket` 的话，它就会把当前进程保留给需要检查的 `socket` 等待队列中
- `socket`  结构 有三块核心区域，分别就是**读缓存，写缓存还有这个等待队列**
- 这个 `select` 函数，它会把当前进程保留到每个需要检查的 `socke` 的等待队列中，就会把当前进程从工作队列里面移除了，移除了之后其实就是挂起了当前线程，然后这个 `select` 函数也就不会再运行了
- **下一个阶段，假设我们客户端往当前服务器发送了数据**，数据通过网线到网卡，网卡再到 `DMA` 硬件的这种方式直接将数据写到内存里面，然后整个过程，`CPU` 是不参与的
- 当传输完成以后，它就会触发网络数据传输完毕的中断程序，这个中断程序它会把 `cpu` 正在执行的进程给顶掉，然后 `cpu` 就会执行咱这个中断程序的逻辑
  - 对应的逻辑是：根据内存中的数据包，然后分析出来数据包是哪个 `socket` 的数据，
  - 同时 `tcp/ip` 它又是保证传输的时候是有端口号的，然后根据端口号就能找到对用的 `socket` 实例，找到 `socket` 实例以后，就会**把数据导入到 `socket` 读缓冲里面**
  - **导入完成以后，它就开始去检查 `socket` 等待队列**，看是不是有等待者，如果有等待者的话，就会把等待者移动到工作队列里面去，中断程序到这一步就执行完了
  - 这样咱们的进程就又回到了工作队列，又有机会获取到 `cpu` 时间片了
- 然后当前进程执行的 `select` 函数再次检查，就会发现这个就绪的 `socket`，就会给就绪的 `socket` 的 `fd` 文件描述符打标记，然后 `select` 函数就执行完了，返回到 `java` 层面就涉及到内核态和用户态的转换，**后面的事情就是轮询检查每一个 `socket` 的 `fd`  是否被打了标记，然后就是处理被打了标记的 `socket` 就 `ok` 了**

## 5、poll 和 select 区别

- 传参不一样
  - **`select` 用的是 `bitmap` ，**它表示需要检查的 `socket` 集合
  - **`poll` 使用的是链表结构，**表示需要检查的 `socket` 集合（主要是为了解决 `socket`  监听长度不超过`1024` 的限制）

## 6、epoll 

- select 和 poll 的共有缺陷

  - 第一个缺陷：`select`  和 `poll`函数，这两系统函数每次调用都需要我们提供给它所有的需要监听的 `socket`  文件描述符集合，而且主线程是死循环调用 `select/poll` 函数的，这里面涉及到用户空间数据到内核空间拷贝的过程

    - 咱们需要监听的socket集合，**数据变化非常小**
    - **每次就一到两个socket_fd需要更改，但是没有办法，因为select和poll函数，只是一个很单纯的函数**
    - **它在kernel层面，不会保留任何的数据信息，所以说每次调用都进行了数据拷贝**

  - 第二个缺陷：

     

    select 和 poll 函数它的返回值都是int整型值，只能代表有几个socket就绪或者有错误了，它没办法表示具体是哪个socket就绪了

    - 这就**导致了程序被唤醒以后，还需要新的一轮系统调用去检查哪个socket是就绪状态的**，然后再进行socket数据处理逻辑，这里走了不少弯路（同时还存在用户态和内核态的切换，这样缺陷就更严重了）

**epoll 就是为了解决这两个问题**

## 7. epoll (…) 实现原理

- epoll 函数

  在内核空间内，它有一个对应的数据结构去存储一些数据，这个数据结构其实就是eventpoll对象

  - 这个`eventpoll` 可以通过一个系统函数`epoll_create()`函数去创建的

- 创建完成之后，系统函数返回一个`eventpoll`对象的id，相当于我们在内核空间开辟了一小块空间，并且我们也知道这块空间的位置

**先说下eventpoll 的数据结构：三块重要的区域**

- 一块是存放**需要监听的socket_fd描述符列表**
- 另一块就是**就绪列表，存放就绪状态的socket信息**
- eventpoll 还有一块空间是`eventpoll` 的**等待队列，这个等待队列保存的就是调用`epoll_wait`的进程**
- **另外呢还提供了两个函数，一个是`epoll_ctl`函数，一个是`epoll_wait`函数**
- **其中存放的socket集合信息采用的是红黑树的数据结构**，socket集合信息经常用增删改查的，这种红黑树再适合不过了，保持了时间复杂度为O(logN)

**epoll_ctl()**

- 它可以根据`eventpoll-id`去增删改内核空间上`eventpoll` 对象的检查列表（socket信息）

**epoll_wait() **

- 它主要的参数是`eventpoll-id` 表示此次系统调用需要检测的`socket_fd`集合，是`eventpoll` 中已经指定好的那些`socket`信息

- `epoll_wait` **默认情况下会阻塞系统的调用线程**，直到`eventpoll` 对象中关联的某个或者某些个socket就绪以后，`epoll_wait`函数才会返回

- 返回值是

  ```
  Int
  ```

  类型的

  - 返回0，表示没有就绪的socket
  - 返回大于0，表示有几个就绪的socket
  - 返回-1表示异常

## 8. eventpoll 对象就绪列表的维护

**select函数调用的流程：**

- socket对象有三块区域
  - 读缓冲区
  - 写缓冲区
  - 等待队列
- select函数调用的时候会把当前进程从工作队列里面拿出来
- **然后把进程引用追加到当前进程关注的每一个socket对象的等待队列中**
- 然后当socket连接的客户端发送完数据之后，数据还是通过硬件DMA的方式把数据写入到内存，然后相应的硬件向CPU发出中断信号,CPU就会让出当前进程位置去执行网络数据就绪的中断程序，
- **这个中断程序就会把内存中的网络数据写入到对应的socket读缓冲区里面，之后把这个socket等待队列中的进程全部移动到工作队列中，再然后select函数返回**

**epoll函数流程非常相似**

- 当我们调用系统函数`epoll_ ctl`时候，比如我们新添加一个需要关注的socket,其实**内核程序会把当前的`eventpoll`对象追加到这个`socket`的等待队列里头**
- 然后当`socket`连接的客户端发送完数据之后，数据还是通过硬件DMA的方式把数据写入到内存，然后相应的硬件向CPU发出中断信号,CPU就会让出当前进程位置去执行网络数据就绪的中断程序，
- **这个中断程序就会把内存中的网络数据写入到对应的`socket`读缓冲区里面，然后它发现这个`socket`的等待队列里头不是进程，而是一个`eventpoll`对象的引用**
- **这个时候呢，他就会根据这个`eventpoll`对象的引用，将当前`socket`的引用追加到eventpoll的就绪链表的末尾**（`eventpoll` 还有一块空间是`eventpoll` 的等待队列，这个等待队列保存的就是调用`epoll_wait`的进程）
- 然后，当中断程序把`socket`的引用追加到就绪列表的末尾之后，就继续检查`eventpoll`对象的等待队列，如果有进程，就会把进程转移到工作队列中
- 转移完毕之后，进程就有获取到CPU执行的时间片了，然后就是调用`epoll_wait` 函数，他这个函数就返回到java层面了

**总结：**

- **eventpoll对象等待队列里面，它有调用epoll_wait(,,,)函数进去的进程**
- **然后再把这个进程，从这个eventpoll的等待队列里面迁移到工作队列里面**

## 9. epoll_wait() 获取就绪的socket

**epoll_wait() 返回值是Int类型的**

- 返回0，表示没有就绪的socket
- 返回大于0，表示有几个就绪的socket
- 返回-1表示异常

**那么获取就绪的socket是怎么实现的呢？**

- `epoll_wait` 函数，**调用的时候**会传入一个`epoll_event`事件数组指针
- `epoll_wait` **函数正常返回之前**，会把就绪的socket事件信息拷贝到这个数组指针里头
- 这样返回到上层程序，就能通过这个数组拿到就绪列表

## 10. epoll_wait 可不可以设置成非阻塞的

- 默认`epoll_wait` 是阻塞的
- 它有一个参数，表示阻塞时间的长度，如果**这个参数设置为0**，表示这个`epoll_wait` 是一个非阻塞调用的
- **每次调用都会去检查就绪列表**